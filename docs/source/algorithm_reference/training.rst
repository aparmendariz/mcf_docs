Training a Modified Causal Forest
=================================

Forest Growing
------------------------------------

Random Forests are an ensemble of decorrelated trees. A regression tree is a non-parametric estimator that splits the data into non-overlapping regions and takes the average of the dependent variable in these strata as prediction for observations sharing the same or similar values of the covariates. The key issue with this approach is that a discrete, non-overlapping data split may be inefficient (no information from neighboring cells are used) and the potential curse of dimensionality may make it difficult to fit a stable split (‘tree’) that has overall good performance. Furthermore, when the number of covariates increases, there are many possible splits of the data, and the computing time needed to form a tree may increase exponentially if all possible splits are considered at each knot. Random Forests solve these problems to some extent by building many decorrelated trees and averaging their predictions. This is achieved by using different random samples of the data for each tree (generated by bootstrapping or subsampling) as well as random subsets of covariates for each splitting decision in an individual leaf of a developing tree. Note that the mcf differs from the causal forest of Wager and Athey (2018) with respect to the splitting criterion when growing the forest. Setting cf_mce_vart to 2, you may switch to the splitting rule of Wager and Athey (2018). Abstracting from the difference in the splitting criterion, the regression forest may seem very much related to the mcf. However, note that the underlying prediction tasks are fundamentally different. The mcf aims to predict causal effects, for which there is no data, and provides (asymptotically) valid inference. To impute the missing data, the mcf requires a causal model. To provide valid inference, the mcf borrows the concept of honesty introduced by Athey and Imbens (2016). For a textbook-like discussion refer to Bodory, Busshoff and Lechner (2022).
