Training a Modified Causal Forest
=================================

Forest Growing
------------------------------------

Random forests are a group of decorrelated regression trees. Due to its non-parametric nature, the regression tree splits the data into non-overlapping strata. Subsequently, it computes the average of the dependent variable within each strata. These averages serve as the prediction for observations with similar covariate values. An issue with this approach is that using discrete, non-overlapping data splits can be inefficient, as it doesn't use information from neighboring data points. Additionally, the curse of dimensionality make it difficult to fit stable splits (or 'trees') with consistently good performance. Moreover, as the number of covariates increases, the number of potential data splits grows significantly. This can lead to exponential increases in computing time if all possible splits are evaluated at each node of the tree.

However, random forests solve these problems (to some extent) by building many decorrelated trees and averaging their predictions. This is achieved by using different random samples of the data to build each tree, generated by bootstrapping or subsampling, as well as random subsets of covariates for each splitting decision in an individual leaf within a developing tree. 

**Note**, the **mcf** differs from the causal forest of `Wager & Athey (2018) <https://doi.org/10.1080/01621459.2017.1319839>`_ with respect to the splitting criterion when growing the forest. 
Setting ``cf_mce_vart`` to ``2``, you may switch to the splitting rule of  `Wager & Athey (2018) <https://doi.org/10.1080/01621459.2017.1319839>`_. 

Apart from the difference in the splitting criterion, the regression forest may seem much related to the **mcf**. 
However, note that the underlying prediction tasks are fundamentally different. 
The **mcf** aims to predict causal effects, for which there is no data, and provides (asymptotically) valid inference. 
To impute the missing data, the **mcf** requires a causal model. 
To provide valid inference, the **mcf** borrows the concept of honesty introduced by `Athey & Imbens (2016) <https://doi.org/10.1073/pnas.1510489113>`_. 
For a textbook-like discussion refer to `Athey & Imbens (2016) <https://www.pnas.org/doi/10.1073/pnas.1510489113>`_.

Implementation
------------------------------------

The number of trees forming the forest is given by the argument cf_boot. 

.. code-block:: python

    my_mcf = ModifiedCausalForest(
        var_y_name="y",
        var_d_name="d",
        var_x_name_ord=["x1", "x2"],
        # Number of trees (default is 1000)
        cf_boot = 500
    )
    my_mcf.gen_dict["outpath"]


As a tree is grown, the algorithm greedily chooses the split which leads to the best possible reduction of the objective function specified in ``cf_mce_vart``. To this end, the following objective criteria are implemented: (i) the outcome mean squared error (MSE), (ii) the outcome MSE and mean correlated errors (MCE), (iii) the variance of the effect, and (iv) the criterion randomly switches between outcome MSE and MCE and penalty functions which are defined under ``cf_p_diff_penalty``. 

The outcome MSE is estimated as the sum of mean squared errors of the outcome regression in each treatment. 
The MCE depends on correlations between treatment states. For this reason, before building the trees, for each observation in each treatment state, the program finds a close ‘neighbor’ in every other treatment state and saves its outcome to then estimate the MCE. How the program matches is governed by the argument ``cf_match_nn_prog_score``. 

The program matches either by outcome scores (one per treatment) or on all covariates by Mahalanobis matching. If there are many covariates, it is advisable to match on outcome scores due to the curse of dimensionality. When performing Mahalanobis matching, a practical issue may be that the required inverse of the covariance matrix is unstable. For this reason the program allows to only use the main diagonal to invert the covariance matrix. This is regulated via the argument ``cf_nn_main_diag_only``. 

Likewise, the program allows for a modification of the splitting rule by adding a penalty to the objective function specified in ``cf_mce_vart``. The idea for deploying a penalty based upon the propensity score is to increase treatment homogeneity within new splits in order to reduce selection bias. Which specific penalty function is used is passed over to the program via the argument ``cf_p_diff_penalty``. Note that from ``cf_mce_vart`` only option (iv) cannot work without the penalty. 

More details on choosing the minimum number of observations in a leaf are given below in section Parameter tuning. Once the forest is settled for the training data, the splits obtained in the training data are transferred to all data subsamples (by treatment state) in the held-out data. 
Finally, the mean of the outcomes in the respective leaf is the prediction.

Below you find a list of the discussed parameters that are relevant for forest growing. Please consult the :py:class:`API <mcf_functions.ModifiedCausalForest>` for more details or additional parameters. 

During tree growth, the algorithm selects splits to optimize the specified objective function in `cf_mce_vart`, including MSE, MSE and MCE, variance of effect, or a random switch between MSE, MCE, and penalty functions defined in `cf_p_diff_penalty`. 

MSE is computed as the sum of squared errors for each treatment, while MCE depends on treatment state correlations, estimated by matching observations using `cf_match_nn_prog_score`. Matching can be based on outcome scores or covariates via Mahalanobis matching, with an option to stabilize covariance matrix inversion using `cf_nn_main_diag_only`.

A penalty function based on propensity score can be added to modify the splitting rule and enhance treatment homogeneity, specified in `cf_p_diff_penalty`. 

The forest's splits from training data are applied to all subsamples in held-out data, and predictions are made by averaging outcomes in each leaf.

For detailed parameter tuning, refer to the API documentation for `ModifiedCausalForest`.

.. dropdown:: Commonly used parameters for forest growing

    +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Parameter                         | Description                                                                                                                                                              |
    +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | ``cf_mce_vart``                   | Splitting rule for tree building, 0 for MSE, 1 for MSE+MCE, 2 for heterogeneity maximization, or 3 for random switching. Default is 1.                          |
    +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | ``cf_p_diff_penalty``             | Penalty function used during tree building, dependent on ``cf_mce_vart``: 0 or 1 for penalty multiplier in terms of variance of y, 2 for penalty multiplier in terms of MSE(y) value function without splits, 3 for probability of using p-score (0-1) None: 0.5 which increases value if balancing tests indicate problems. Default is None. |
    +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | ``cf_match_nn_prog_score``        | Choice of method of nearest neighbour matching. True: Prognostic scores. False: Inverse of covariance matrix of features. Default (or None) is True.               |
    +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | ``cf_nn_main_diag_only``          | Nearest neighbour matching: Use main diagonal of covariance matrix only. Only relevant if match_nn_prog_score == False. Default (or None) is False.                 |
    +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+



